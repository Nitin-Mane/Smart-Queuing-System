{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step1: Create the Python Script\n",
    "\n",
    "In the cell below, you will need to complete the Python script and run the cell to generate the file using the magic `%%writefile` command. Your main task is to complete the following methods for the `PersonDetect` class:\n",
    "* `load_model`\n",
    "* `predict`\n",
    "* `draw_outputs`\n",
    "* `preprocess_outputs`\n",
    "* `preprocess_inputs`\n",
    "\n",
    "For your reference, here are all the arguments used for the argument parser in the command line:\n",
    "* `--model`:  The file path of the pre-trained IR model, which has been pre-processed using the model optimizer. There is automated support built in this argument to support both FP32 and FP16 models targeting different hardware.\n",
    "* `--device`: The type of hardware you want to load the model on (CPU, GPU, MYRIAD, HETERO:FPGA,CPU)\n",
    "* `--video`: The file path of the input video.\n",
    "* `--output_path`: The location where the output stats and video file with inference needs to be stored (results/[device]).\n",
    "* `--max_people`: The max number of people in queue before directing a person to another queue.\n",
    "* `--threshold`: The probability threshold value for the person detection. Optional arg; default value is 0.60."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting person_detect.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile person_detect.py\n",
    "\n",
    "#########################################################################################################################\n",
    "##\n",
    "##                                          Person Detection script \n",
    "##\n",
    "#########################################################################################################################\n",
    "\n",
    "# Initating the libraries \n",
    "import numpy as np\n",
    "import time\n",
    "from openvino.inference_engine import IENetwork, IECore\n",
    "import os\n",
    "import cv2\n",
    "import argparse\n",
    "import sys\n",
    "\n",
    "## Queue class\n",
    "\n",
    "class Queue:\n",
    "    '''\n",
    "    Class for dealing with queues requests\n",
    "    \n",
    "    Performs basic operations for queues like adding to a queue, getting the queues \n",
    "    and checking the coordinates for queues.\n",
    "    \n",
    "    Labels - add_queue, get_queues, check_recMat\n",
    "    \n",
    "    the queues are used to the generates the coordinates of the tracking object and perform frames operations. \n",
    "    \n",
    "    '''\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Initalizing the queue algorithm\n",
    "        '''\n",
    "        self.queues=[]\n",
    "\n",
    "    def add_queue(self, points):\n",
    "        '''\n",
    "        Input: Points \n",
    "        \n",
    "        Output: list (Points)\n",
    "        Adding the points data in list\n",
    "        '''\n",
    "        self.queues.append(points)\n",
    "\n",
    "    def get_queues(self, image):\n",
    "        '''\n",
    "        Input: Image \n",
    "        \n",
    "        Output: frames \n",
    "        \n",
    "        The queues generated from images are passed to the yield of frames\n",
    "        '''\n",
    "        for q in self.queues:\n",
    "            x_min, y_min, x_max, y_max=q\n",
    "            frame=image[y_min:y_max, x_min:x_max]\n",
    "            yield (frame)\n",
    "    \n",
    "    def check_recMat(self, recMat):\n",
    "        '''\n",
    "        input: recMat\n",
    "        \n",
    "        Output: d \n",
    "        \n",
    "        the recMat find the range queues in the data and check the coordinates of the input data. \n",
    "        '''\n",
    "        d={k+1:0 for k in range(len(self.queues))}\n",
    "        \n",
    "        for coord in recMat:\n",
    "            for i, q in enumerate(self.queues):\n",
    "                if coord[0]>q[0] and coord[2]<q[2]:\n",
    "                    d[i+1]+=1\n",
    "        return (d)\n",
    "\n",
    "## Person Detection Class \n",
    "    \n",
    "class PersonDetect:\n",
    "    '''\n",
    "    Class for the Person Detection Model.\n",
    "    \n",
    "    Program for data preprocessing and detecting of person using Intel Open Model Zoo. \n",
    "    \n",
    "    Input_Attributes:\n",
    "        \n",
    "        model_weights: A model weights path in bin format.\n",
    "        model_structure: A model structure path in xml format.\n",
    "        device: A device which perform task on the processor {CPU, GPU, Myraid, FPGA} .\n",
    "        IEcore: coreCore represents an Inference Engine.\n",
    "        model: Load model object for Intermediate Representation.\n",
    "        input_name: A input list for the image or the video.\n",
    "        input_shape: the input shape is the size or resolution the tuple input.\n",
    "        output_shape: the output shape is the generated pre-process coordinate in tuple shape.\n",
    "        output_name: the output names is the out source list of image or frames data.\n",
    "        threshold: A threshold value is set the floting limits of the .\n",
    "        \n",
    "        --model ${Load model} \n",
    "        --device ${Select the device type} \n",
    "        --video ${Input video file (mp4, mpeg, MKV)} \n",
    "        --queue_param ${setting queue parameter for limit} \n",
    "        --output_path ${Set-up output path directory}\n",
    "        --max_people ${Limiting maximum people} \n",
    "\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, model_name, device, threshold=0.60):\n",
    "        '''\n",
    "        Initalizing the code for the people detection in queue. \n",
    "        '''\n",
    "        # Initialize the model input parameters\n",
    "        self.model_weights=model_name+'.bin'\n",
    "        self.model_structure=model_name+'.xml'\n",
    "        self.device=device\n",
    "        self.threshold=threshold\n",
    "        \n",
    "        \n",
    "        ## Initialize frame class variables\n",
    "        # Setting the values to zeros \n",
    "        self.x = (0,0) #coordinate x shape \n",
    "        self.y = (0,0) #coordintate y shape\n",
    "        self.w = (0.0) #frame width \n",
    "        self.h = (0.0) #frame height \n",
    "        self.cnt = contours[0] # countour set zero\n",
    "        self.input_frame = None # set input frame to None \n",
    "        self.case_frame = None # set case frame to None for later use\n",
    "        self.model= None # set model for None \n",
    "        self.ex_model= None #set executable model network to None\n",
    "        \n",
    "        # Initialize the inference engine for the running the code. (issue - model was not loading using IENetwork)\n",
    "        # [Solution](https://docs.openvinotoolkit.org/latest/ie_python_api/classie__api_1_1IECore.html#afe73d64ddd115a41f5acc0d31031f52b)\n",
    "        self.core= IECore() # Inference Engine Plugin\n",
    "        \n",
    "        try:\n",
    "            self.model= self.core.read_network(self.model_structure, self.model_weights) # use read_network instead of IEnetwork\n",
    "            #self.model=IENetwork(self.model_structure, self.model_weights)\n",
    "        except Exception as e:\n",
    "            raise ValueError(\"Could not Initialise the network. Have you enterred the correct model path?\")\n",
    "            \n",
    "        # Getting the input layer for the model\n",
    "        '''\n",
    "        [doc](https://docs.openvinotoolkit.org/2018_R5/_ie_bridges_python_docs_api_overview.html)\n",
    "        '''\n",
    "        self.input_name=next(iter(self.model.inputs))\n",
    "        self.input_shape=self.model.inputs[self.input_name].shape\n",
    "        \n",
    "        #self.output_shape=self(model.output[self_name])\n",
    "        self.output_name=next(iter(self.model.outputs))\n",
    "        self.output_shape=self.model.outputs[self.output_name].shape\n",
    "\n",
    "    def load_model(self):\n",
    "        '''\n",
    "        TODO: This method needs to be completed by you\n",
    "        \n",
    "        Load the inference model in function. \n",
    "        \n",
    "        [doc](https://docs.openvinotoolkit.org/latest/classInferenceEngine_1_1Core.html)\n",
    "        \n",
    "        load network: \n",
    "                    network = model file \n",
    "                    device_name = device type \n",
    "                    num_request = 1 set default\n",
    "        \n",
    "        '''\n",
    "        # Load the model network\n",
    "        self.ex_model= self.core.load_network(network=self.model,device_name=self.device,num_requests=1)\n",
    "        \n",
    "        #self.core = IECore()\n",
    "        #self.net = self.core.load_network(network=self.model, device_name=self.device, num_requests=1)\n",
    "    \n",
    "    \n",
    "    def asynch_request_get(self, input_image):\n",
    "        '''\n",
    "        Start an asynchronous request\n",
    "        It is about running the primary application thread seperate \n",
    "        \n",
    "        input: input_image \n",
    "        \n",
    "        Output: result directory \n",
    "        '''\n",
    "        \n",
    "        self.ex_model.start_async(request_id=0,inputs=input_image) # asynchrnous model start\n",
    "        \n",
    "        #the request get hold untill is gets complete operation \n",
    "        \n",
    "        status = self.ex_model.requests[0].wait(-1) # keeping the status for the model request\n",
    "        \n",
    "        if status==0:\n",
    "            # Extract and return the output results\n",
    "            result = self.ex_model.requests[0].outputs[self.output_name] # outpu_name request throgh model\n",
    "            \n",
    "            return (result)\n",
    "        \n",
    "   \n",
    "    def predict(self, image):\n",
    "        '''\n",
    "        TODO: This method needs to be completed by you\n",
    "        \n",
    "        Prediction alogorithm \n",
    "        the program use for pre-processing input image and getting the coordinate data and output image. \n",
    "        \n",
    "        Input: image \n",
    "        \n",
    "        Output: recMat and output_image \n",
    "        '''\n",
    "        \n",
    "        self.input_frame = image\n",
    "        \n",
    "        #the input frame is passed to the prediction \n",
    "        \n",
    "        # Inference input process \n",
    "        #input_frame={input_name: input_image} \n",
    "        \n",
    "        self.case_frame = self.preprocess_input(self.input_frame)\n",
    "        #infer_request_handle = self.net.start_async(request_id=0, inputs=input_frame)\n",
    "        input_image={self.input_name: self.case_frame}\n",
    "        \n",
    "        #the preprocessing of the image data is extracted\n",
    "        #infer_status = infer_request_handle.wait()\n",
    "        #outputs = infer_request_handle.outputs[self.output_image]\n",
    "        \n",
    "        result = self.asynch_request_get(input_image) # result of the prediction\n",
    "        \n",
    "        recMat, output_image = self.preprocess_outputs(result) # coordinates of the person detected\n",
    "        \n",
    "        \n",
    "        return (recMat,output_image)\n",
    "    \n",
    "    def draw_outputs(self, recMat, image):\n",
    "        '''\n",
    "        TODO: This method needs to be completed by you\n",
    "        \n",
    "        The draw output algoritm generate the bounding boxes in the image with respect to the coordinate image. \n",
    "        \n",
    "        [doc](https://docs.opencv.org/3.4/da/d0c/tutorial_bounding_rects_circles.html)\n",
    "        \n",
    "        input: recMat and image \n",
    "        \n",
    "        output: image outcome using opencv. \n",
    "        '''\n",
    "        \n",
    "        pass1 = (recMat[0],recMat[1])\n",
    "        pass2 = (recMat[2],recMat[3])\n",
    "        \n",
    "        #cv2.drawContours(img_copy, self.cnt, contourIdx = -1, \n",
    "        #                 color = (255, 0, 0), thickness = 2)\n",
    "        \n",
    "        # Drawing bounding boxes on the image input\n",
    "        \n",
    "        cv2.rectangle(image, pass1, pass2, (255, 0, 0) , 2)\n",
    "        #x,y,w,h = cv2.boundingRect(self.cnt)\n",
    "        #img = cv2.rectangle(image,(x,y),(x+w,y+h),(0,255,0),2)\n",
    "        \n",
    "        return\n",
    "\n",
    "    def preprocess_outputs(self, outputs):\n",
    "        '''\n",
    "        TODO: This method needs to be completed by you\n",
    "        \n",
    "        Pre-processing is the image processing techniques to applying the image processing techniques and adding box output\n",
    "        for reference. The threshold is set at enhances image features at detected person. \n",
    "        \n",
    "        input: outputs\n",
    "        \n",
    "        output: coordinates, input_frame\n",
    "        '''\n",
    "        \n",
    "        coordinates=list()\n",
    "        \n",
    "        #In the box in represent as {outputs[0][0]}:\n",
    "        \n",
    "        for b in range (len(outputs[0][0])):\n",
    "            box = outputs[0][0][b]\n",
    "            confidence = box[2]\n",
    "            if confidence > self.threshold:\n",
    "                x_min,x_max = map(lambda b : int(b*self.w), [box[3],box[5]]) # x coordinate box\n",
    "                y_min,y_max = map(lambda b : int(b*self.h), [box[4],box[6]]) # y coordinate box\n",
    "                coordinates.append([x_min,y_min,x_max,y_max])\n",
    "                recMat = [x_min,y_min,x_max,y_max]\n",
    "                self.draw_outputs(recMat, self.input_frame)\n",
    "                \n",
    "        return (coordinates, self.input_frame)\n",
    "\n",
    "    def preprocess_input(self, image):\n",
    "        '''\n",
    "        TODO: This method needs to be completed by you\n",
    "        \n",
    "        Pre-processing input takes image and pass the feature operation for providing the input parameter shape to model. \n",
    "        this is used to resize the input frame. \n",
    "        \n",
    "        input: image \n",
    "        \n",
    "        output: pass_image\n",
    "        '''\n",
    "        # reference to the main code script\n",
    "        # Pre-process of the input image \n",
    "        \n",
    "        _width=self.input_shape[3]\n",
    "        _height=self.input_shape[2]\n",
    "        \n",
    "        pass_image = cv2.resize(image, (_width, _height))\n",
    "        # Image processing technique\n",
    "        pass_image = pass_image.transpose((2,0,1))\n",
    "        pass_image = pass_image.reshape(self.input_shape[0], self.input_shape[1], _height, _width)\n",
    "        \n",
    "        return (pass_image)\n",
    "\n",
    "## Main file processing script\n",
    "\n",
    "def main(args):\n",
    "    \n",
    "    model=args.model\n",
    "    device=args.device\n",
    "    video_file=args.video\n",
    "    max_people=args.max_people\n",
    "    threshold=args.threshold\n",
    "    output_path=args.output_path\n",
    "\n",
    "    start_model_load_time=time.time()\n",
    "    pd = PersonDetect(model, device, threshold)\n",
    "    pd.load_model()\n",
    "    total_model_load_time = time.time() - start_model_load_time\n",
    "\n",
    "    queue=Queue()\n",
    "    \n",
    "    try:\n",
    "        queue_param=np.load(args.queue_param)\n",
    "        for q in queue_param:\n",
    "            queue.add_queue(q)\n",
    "    except:\n",
    "        print(\"error loading queue param file\")\n",
    "\n",
    "    try:\n",
    "        cap=cv2.VideoCapture(video_file)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Cannot locate video file: \"+ video_file)\n",
    "    except Exception as e:\n",
    "        print(\"Something else went wrong with the video file: \", e)\n",
    "        \n",
    "    initial_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    initial_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    pd.w = initial_w\n",
    "    pd.h = initial_h\n",
    "    video_len = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    out_video = cv2.VideoWriter(os.path.join(output_path, 'output_video.mp4'), cv2.VideoWriter_fourcc(*'avc1'), fps, (initial_w, initial_h))\n",
    "    \n",
    "    counter=0\n",
    "    start_inference_time=time.time()\n",
    "\n",
    "    try:\n",
    "        while cap.isOpened():\n",
    "            ret, frame=cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            counter+=1\n",
    "            \n",
    "            recMat, image= pd.predict(frame)\n",
    "            num_people= queue.check_recMat(recMat)\n",
    "            print(f\"Total People in frame = {len(recMat)}\")\n",
    "            print(f\"Number of people in queue = {num_people}\")\n",
    "            out_text=\"\"\n",
    "            y_pixel=25\n",
    "            \n",
    "            for k, v in num_people.items():\n",
    "                out_text += f\"No. of People in Queue {k} is {v} \"\n",
    "                if v >= int(max_people):\n",
    "                    out_text += f\" Queue full; Please move to next Queue \"\n",
    "                cv2.putText(image, out_text, (15, y_pixel), cv2.FONT_HERSHEY_COMPLEX, 1, (0, 255, 60), 2)\n",
    "                out_text=\"\"\n",
    "                y_pixel+=40\n",
    "            out_video.write(image)\n",
    "            \n",
    "        total_time=time.time()-start_inference_time\n",
    "        total_inference_time=round(total_time, 1)\n",
    "        fps=counter/total_inference_time\n",
    "\n",
    "        with open(os.path.join(output_path, 'stats.txt'), 'w') as f:\n",
    "            f.write(str(total_inference_time)+'\\n')\n",
    "            f.write(str(fps)+'\\n')\n",
    "            f.write(str(total_model_load_time)+'\\n')\n",
    "\n",
    "        out_video.release()\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "    \n",
    "    except Exception as e:\n",
    "        \n",
    "        print (fps)\n",
    "        print (total_inference_time)\n",
    "        print(\"Could not run Inference: \", e)\n",
    "\n",
    "## Main script for execution \n",
    "        \n",
    "if __name__=='__main__':\n",
    "    \n",
    "    parser=argparse.ArgumentParser()\n",
    "    \n",
    "    parser.add_argument('--model', required=True, type=str)\n",
    "    parser.add_argument('--device', default='CPU', type=str)\n",
    "    parser.add_argument('--video', default=None, type=str)\n",
    "    parser.add_argument('--queue_param', default=None, type=str)\n",
    "    parser.add_argument('--output_path', default='/results', type=str)\n",
    "    parser.add_argument('--max_people', default=2, type=int)\n",
    "    parser.add_argument('--threshold', default=0.60, type=float)\n",
    "    \n",
    "    args=parser.parse_args()\n",
    "\n",
    "    \n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Step\n",
    "\n",
    "Now that you've run the above cell and created your Python script, you will create your job submission shell script in the next workspace.\n",
    "\n",
    "**Note**: As a reminder, if you need to make any changes to the Python script, you can come back to this workspace to edit and run the above cell to overwrite the file with your changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
